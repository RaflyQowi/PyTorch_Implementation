# PyTorch Implementation

This repository contains implementations of various deep learning models and concepts using PyTorch. It serves as a learning resource and reference for those interested in working with PyTorch for machine learning tasks.

## 1. PyTorch Basic

The "PyTorch Basic" directory includes Python scripts that I wrote during my learning journey. These scripts cover fundamental concepts in deep learning and PyTorch, including:

- **Create Simple FNN:** Implementation of a simple Feedforward Neural Network.
- **Create Simple CNN:** Implementation of a basic Convolutional Neural Network.
- **Create Simple RNN:** Implementation of a basic Recurrent Neural Network.
- **Create Bidirectional RNN:** Implementation of a Bidirectional Recurrent Neural Network.
- **Save Load Model:** Example scripts demonstrating how to save and load PyTorch models.
- **Transfer Learning:** Example of transfer learning using pre-trained models.
- **Custom Dataset:** Implementation of a custom dataset for PyTorch.
- **Imbalance Dataset:** Handling imbalanced datasets.

## 2. CNN Model Architecture from Scratch using PyTorch

The "CNN Model Architecture from Scratch using PyTorch" directory focuses on implementing well-known Convolutional Neural Network architectures:

### LeNet5 Architecture

- Implementation of the LeNet5 architecture, a pioneering CNN model by Yann LeCun.

### Vgg Architecture

- Implementation of the Vgg architecture, known for its simplicity and effectiveness.

### GoogleNet Architecture

- Implementation of the GoogleNet (Inception) architecture, featuring inception modules.

### ResNet Architecture

- Implementation of the Residual Network (ResNet) architecture.

### EfficientNet Architecture

- Implementation of the EfficientNet architecture.

## Usage

Feel free to explore the directories and use the provided scripts for learning, experimenting, or as a reference for your own projects.

## Acknowledgments

I appreciate the resources and tutorials that contributed to my learning process. Credits to the respective authors and contributors.

Happy coding and learning!

---
